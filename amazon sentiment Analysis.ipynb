{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68406632",
   "metadata": {},
   "source": [
    "# Amazon sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0071fee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08ee00ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(r'C:\\Users\\Sabin Sapkota\\Desktop\\amazon\\Reviews.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b7922b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568454, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87ec8e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator',\n",
       "       'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'Text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce352d6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                         int64\n",
       "ProductId                 object\n",
       "UserId                    object\n",
       "ProfileName               object\n",
       "HelpfulnessNumerator       int64\n",
       "HelpfulnessDenominator     int64\n",
       "Score                      int64\n",
       "Time                       int64\n",
       "Summary                   object\n",
       "Text                      object\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6c88432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                         0\n",
       "ProductId                  0\n",
       "UserId                     0\n",
       "ProfileName               16\n",
       "HelpfulnessNumerator       0\n",
       "HelpfulnessDenominator     0\n",
       "Score                      0\n",
       "Time                       0\n",
       "Summary                   27\n",
       "Text                       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86a8d522",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['helpfull%']=np.where(df['HelpfulnessDenominator']>0,df.HelpfulnessNumerator/df.HelpfulnessDenominator,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bd251bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['%upvote']=pd.cut(df['helpfull%'],bins=[-1,0,0.2,0.4,0.6,0.8,1],labels=['empty','0-20%','20-40%','40-60%','60-80%','80-100%'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7daf262",
   "metadata": {},
   "source": [
    "# Analyse upvotes for different score ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7b023ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>helpfull%</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Score</th>\n",
       "      <th>%upvote</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">1</th>\n",
       "      <th>empty</th>\n",
       "      <td>8060</td>\n",
       "      <td>8060</td>\n",
       "      <td>8060</td>\n",
       "      <td>8060</td>\n",
       "      <td>8060</td>\n",
       "      <td>8060</td>\n",
       "      <td>8060</td>\n",
       "      <td>8060</td>\n",
       "      <td>8060</td>\n",
       "      <td>8060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0-20%</th>\n",
       "      <td>2338</td>\n",
       "      <td>2338</td>\n",
       "      <td>2338</td>\n",
       "      <td>2338</td>\n",
       "      <td>2338</td>\n",
       "      <td>2338</td>\n",
       "      <td>2338</td>\n",
       "      <td>2338</td>\n",
       "      <td>2338</td>\n",
       "      <td>2338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20-40%</th>\n",
       "      <td>4649</td>\n",
       "      <td>4649</td>\n",
       "      <td>4649</td>\n",
       "      <td>4649</td>\n",
       "      <td>4649</td>\n",
       "      <td>4649</td>\n",
       "      <td>4649</td>\n",
       "      <td>4649</td>\n",
       "      <td>4649</td>\n",
       "      <td>4649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40-60%</th>\n",
       "      <td>6586</td>\n",
       "      <td>6586</td>\n",
       "      <td>6586</td>\n",
       "      <td>6586</td>\n",
       "      <td>6586</td>\n",
       "      <td>6586</td>\n",
       "      <td>6586</td>\n",
       "      <td>6586</td>\n",
       "      <td>6586</td>\n",
       "      <td>6586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60-80%</th>\n",
       "      <td>5838</td>\n",
       "      <td>5838</td>\n",
       "      <td>5838</td>\n",
       "      <td>5836</td>\n",
       "      <td>5838</td>\n",
       "      <td>5838</td>\n",
       "      <td>5838</td>\n",
       "      <td>5838</td>\n",
       "      <td>5838</td>\n",
       "      <td>5838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80-100%</th>\n",
       "      <td>12531</td>\n",
       "      <td>12531</td>\n",
       "      <td>12531</td>\n",
       "      <td>12531</td>\n",
       "      <td>12531</td>\n",
       "      <td>12531</td>\n",
       "      <td>12531</td>\n",
       "      <td>12531</td>\n",
       "      <td>12531</td>\n",
       "      <td>12531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">2</th>\n",
       "      <th>empty</th>\n",
       "      <td>4234</td>\n",
       "      <td>4234</td>\n",
       "      <td>4234</td>\n",
       "      <td>4234</td>\n",
       "      <td>4234</td>\n",
       "      <td>4234</td>\n",
       "      <td>4234</td>\n",
       "      <td>4234</td>\n",
       "      <td>4234</td>\n",
       "      <td>4234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0-20%</th>\n",
       "      <td>762</td>\n",
       "      <td>762</td>\n",
       "      <td>762</td>\n",
       "      <td>762</td>\n",
       "      <td>762</td>\n",
       "      <td>762</td>\n",
       "      <td>762</td>\n",
       "      <td>737</td>\n",
       "      <td>762</td>\n",
       "      <td>762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20-40%</th>\n",
       "      <td>1618</td>\n",
       "      <td>1618</td>\n",
       "      <td>1618</td>\n",
       "      <td>1618</td>\n",
       "      <td>1618</td>\n",
       "      <td>1618</td>\n",
       "      <td>1618</td>\n",
       "      <td>1618</td>\n",
       "      <td>1618</td>\n",
       "      <td>1618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40-60%</th>\n",
       "      <td>3051</td>\n",
       "      <td>3051</td>\n",
       "      <td>3051</td>\n",
       "      <td>3051</td>\n",
       "      <td>3051</td>\n",
       "      <td>3051</td>\n",
       "      <td>3051</td>\n",
       "      <td>3051</td>\n",
       "      <td>3051</td>\n",
       "      <td>3051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60-80%</th>\n",
       "      <td>2486</td>\n",
       "      <td>2486</td>\n",
       "      <td>2486</td>\n",
       "      <td>2486</td>\n",
       "      <td>2486</td>\n",
       "      <td>2486</td>\n",
       "      <td>2486</td>\n",
       "      <td>2486</td>\n",
       "      <td>2486</td>\n",
       "      <td>2486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80-100%</th>\n",
       "      <td>7014</td>\n",
       "      <td>7014</td>\n",
       "      <td>7014</td>\n",
       "      <td>7014</td>\n",
       "      <td>7014</td>\n",
       "      <td>7014</td>\n",
       "      <td>7014</td>\n",
       "      <td>7014</td>\n",
       "      <td>7014</td>\n",
       "      <td>7014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">3</th>\n",
       "      <th>empty</th>\n",
       "      <td>5062</td>\n",
       "      <td>5062</td>\n",
       "      <td>5062</td>\n",
       "      <td>5062</td>\n",
       "      <td>5062</td>\n",
       "      <td>5062</td>\n",
       "      <td>5062</td>\n",
       "      <td>5062</td>\n",
       "      <td>5062</td>\n",
       "      <td>5062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0-20%</th>\n",
       "      <td>474</td>\n",
       "      <td>474</td>\n",
       "      <td>474</td>\n",
       "      <td>474</td>\n",
       "      <td>474</td>\n",
       "      <td>474</td>\n",
       "      <td>474</td>\n",
       "      <td>474</td>\n",
       "      <td>474</td>\n",
       "      <td>474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20-40%</th>\n",
       "      <td>1506</td>\n",
       "      <td>1506</td>\n",
       "      <td>1506</td>\n",
       "      <td>1506</td>\n",
       "      <td>1506</td>\n",
       "      <td>1506</td>\n",
       "      <td>1506</td>\n",
       "      <td>1506</td>\n",
       "      <td>1506</td>\n",
       "      <td>1506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40-60%</th>\n",
       "      <td>3384</td>\n",
       "      <td>3384</td>\n",
       "      <td>3384</td>\n",
       "      <td>3384</td>\n",
       "      <td>3384</td>\n",
       "      <td>3384</td>\n",
       "      <td>3384</td>\n",
       "      <td>3384</td>\n",
       "      <td>3384</td>\n",
       "      <td>3384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60-80%</th>\n",
       "      <td>2754</td>\n",
       "      <td>2754</td>\n",
       "      <td>2754</td>\n",
       "      <td>2754</td>\n",
       "      <td>2754</td>\n",
       "      <td>2754</td>\n",
       "      <td>2754</td>\n",
       "      <td>2754</td>\n",
       "      <td>2754</td>\n",
       "      <td>2754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80-100%</th>\n",
       "      <td>11037</td>\n",
       "      <td>11037</td>\n",
       "      <td>11037</td>\n",
       "      <td>11037</td>\n",
       "      <td>11037</td>\n",
       "      <td>11037</td>\n",
       "      <td>11037</td>\n",
       "      <td>11036</td>\n",
       "      <td>11037</td>\n",
       "      <td>11037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">4</th>\n",
       "      <th>empty</th>\n",
       "      <td>4780</td>\n",
       "      <td>4780</td>\n",
       "      <td>4780</td>\n",
       "      <td>4780</td>\n",
       "      <td>4780</td>\n",
       "      <td>4780</td>\n",
       "      <td>4780</td>\n",
       "      <td>4780</td>\n",
       "      <td>4780</td>\n",
       "      <td>4780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0-20%</th>\n",
       "      <td>116</td>\n",
       "      <td>116</td>\n",
       "      <td>116</td>\n",
       "      <td>116</td>\n",
       "      <td>116</td>\n",
       "      <td>116</td>\n",
       "      <td>116</td>\n",
       "      <td>116</td>\n",
       "      <td>116</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20-40%</th>\n",
       "      <td>909</td>\n",
       "      <td>909</td>\n",
       "      <td>909</td>\n",
       "      <td>909</td>\n",
       "      <td>909</td>\n",
       "      <td>909</td>\n",
       "      <td>909</td>\n",
       "      <td>909</td>\n",
       "      <td>909</td>\n",
       "      <td>909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40-60%</th>\n",
       "      <td>3185</td>\n",
       "      <td>3185</td>\n",
       "      <td>3185</td>\n",
       "      <td>3185</td>\n",
       "      <td>3185</td>\n",
       "      <td>3185</td>\n",
       "      <td>3185</td>\n",
       "      <td>3185</td>\n",
       "      <td>3185</td>\n",
       "      <td>3185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60-80%</th>\n",
       "      <td>2941</td>\n",
       "      <td>2941</td>\n",
       "      <td>2941</td>\n",
       "      <td>2941</td>\n",
       "      <td>2941</td>\n",
       "      <td>2941</td>\n",
       "      <td>2941</td>\n",
       "      <td>2941</td>\n",
       "      <td>2941</td>\n",
       "      <td>2941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80-100%</th>\n",
       "      <td>26707</td>\n",
       "      <td>26707</td>\n",
       "      <td>26707</td>\n",
       "      <td>26707</td>\n",
       "      <td>26707</td>\n",
       "      <td>26707</td>\n",
       "      <td>26707</td>\n",
       "      <td>26707</td>\n",
       "      <td>26707</td>\n",
       "      <td>26707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">5</th>\n",
       "      <th>empty</th>\n",
       "      <td>11638</td>\n",
       "      <td>11638</td>\n",
       "      <td>11638</td>\n",
       "      <td>11638</td>\n",
       "      <td>11638</td>\n",
       "      <td>11638</td>\n",
       "      <td>11638</td>\n",
       "      <td>11638</td>\n",
       "      <td>11638</td>\n",
       "      <td>11638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0-20%</th>\n",
       "      <td>432</td>\n",
       "      <td>432</td>\n",
       "      <td>432</td>\n",
       "      <td>432</td>\n",
       "      <td>432</td>\n",
       "      <td>432</td>\n",
       "      <td>432</td>\n",
       "      <td>432</td>\n",
       "      <td>432</td>\n",
       "      <td>432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20-40%</th>\n",
       "      <td>2275</td>\n",
       "      <td>2275</td>\n",
       "      <td>2275</td>\n",
       "      <td>2275</td>\n",
       "      <td>2275</td>\n",
       "      <td>2275</td>\n",
       "      <td>2275</td>\n",
       "      <td>2275</td>\n",
       "      <td>2275</td>\n",
       "      <td>2275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40-60%</th>\n",
       "      <td>10312</td>\n",
       "      <td>10312</td>\n",
       "      <td>10312</td>\n",
       "      <td>10312</td>\n",
       "      <td>10312</td>\n",
       "      <td>10312</td>\n",
       "      <td>10312</td>\n",
       "      <td>10312</td>\n",
       "      <td>10312</td>\n",
       "      <td>10312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60-80%</th>\n",
       "      <td>11060</td>\n",
       "      <td>11060</td>\n",
       "      <td>11060</td>\n",
       "      <td>11060</td>\n",
       "      <td>11060</td>\n",
       "      <td>11060</td>\n",
       "      <td>11060</td>\n",
       "      <td>11060</td>\n",
       "      <td>11060</td>\n",
       "      <td>11060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80-100%</th>\n",
       "      <td>140661</td>\n",
       "      <td>140661</td>\n",
       "      <td>140661</td>\n",
       "      <td>140659</td>\n",
       "      <td>140661</td>\n",
       "      <td>140661</td>\n",
       "      <td>140661</td>\n",
       "      <td>140661</td>\n",
       "      <td>140661</td>\n",
       "      <td>140661</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Id  ProductId  UserId  ProfileName  HelpfulnessNumerator  \\\n",
       "Score %upvote                                                                 \n",
       "1     empty      8060       8060    8060         8060                  8060   \n",
       "      0-20%      2338       2338    2338         2338                  2338   \n",
       "      20-40%     4649       4649    4649         4649                  4649   \n",
       "      40-60%     6586       6586    6586         6586                  6586   \n",
       "      60-80%     5838       5838    5838         5836                  5838   \n",
       "      80-100%   12531      12531   12531        12531                 12531   \n",
       "2     empty      4234       4234    4234         4234                  4234   \n",
       "      0-20%       762        762     762          762                   762   \n",
       "      20-40%     1618       1618    1618         1618                  1618   \n",
       "      40-60%     3051       3051    3051         3051                  3051   \n",
       "      60-80%     2486       2486    2486         2486                  2486   \n",
       "      80-100%    7014       7014    7014         7014                  7014   \n",
       "3     empty      5062       5062    5062         5062                  5062   \n",
       "      0-20%       474        474     474          474                   474   \n",
       "      20-40%     1506       1506    1506         1506                  1506   \n",
       "      40-60%     3384       3384    3384         3384                  3384   \n",
       "      60-80%     2754       2754    2754         2754                  2754   \n",
       "      80-100%   11037      11037   11037        11037                 11037   \n",
       "4     empty      4780       4780    4780         4780                  4780   \n",
       "      0-20%       116        116     116          116                   116   \n",
       "      20-40%      909        909     909          909                   909   \n",
       "      40-60%     3185       3185    3185         3185                  3185   \n",
       "      60-80%     2941       2941    2941         2941                  2941   \n",
       "      80-100%   26707      26707   26707        26707                 26707   \n",
       "5     empty     11638      11638   11638        11638                 11638   \n",
       "      0-20%       432        432     432          432                   432   \n",
       "      20-40%     2275       2275    2275         2275                  2275   \n",
       "      40-60%    10312      10312   10312        10312                 10312   \n",
       "      60-80%    11060      11060   11060        11060                 11060   \n",
       "      80-100%  140661     140661  140661       140659                140661   \n",
       "\n",
       "               HelpfulnessDenominator    Time  Summary    Text  helpfull%  \n",
       "Score %upvote                                                              \n",
       "1     empty                      8060    8060     8060    8060       8060  \n",
       "      0-20%                      2338    2338     2338    2338       2338  \n",
       "      20-40%                     4649    4649     4649    4649       4649  \n",
       "      40-60%                     6586    6586     6586    6586       6586  \n",
       "      60-80%                     5838    5838     5838    5838       5838  \n",
       "      80-100%                   12531   12531    12531   12531      12531  \n",
       "2     empty                      4234    4234     4234    4234       4234  \n",
       "      0-20%                       762     762      737     762        762  \n",
       "      20-40%                     1618    1618     1618    1618       1618  \n",
       "      40-60%                     3051    3051     3051    3051       3051  \n",
       "      60-80%                     2486    2486     2486    2486       2486  \n",
       "      80-100%                    7014    7014     7014    7014       7014  \n",
       "3     empty                      5062    5062     5062    5062       5062  \n",
       "      0-20%                       474     474      474     474        474  \n",
       "      20-40%                     1506    1506     1506    1506       1506  \n",
       "      40-60%                     3384    3384     3384    3384       3384  \n",
       "      60-80%                     2754    2754     2754    2754       2754  \n",
       "      80-100%                   11037   11037    11036   11037      11037  \n",
       "4     empty                      4780    4780     4780    4780       4780  \n",
       "      0-20%                       116     116      116     116        116  \n",
       "      20-40%                      909     909      909     909        909  \n",
       "      40-60%                     3185    3185     3185    3185       3185  \n",
       "      60-80%                     2941    2941     2941    2941       2941  \n",
       "      80-100%                   26707   26707    26707   26707      26707  \n",
       "5     empty                     11638   11638    11638   11638      11638  \n",
       "      0-20%                       432     432      432     432        432  \n",
       "      20-40%                     2275    2275     2275    2275       2275  \n",
       "      40-60%                    10312   10312    10312   10312      10312  \n",
       "      60-80%                    11060   11060    11060   11060      11060  \n",
       "      80-100%                  140661  140661   140661  140661     140661  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['Score','%upvote']).agg('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b252ec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator',\n",
       "       'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'Text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ef0147",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b6de68c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 1, 4, 2, 3], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Score'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e98ced58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>helpfull%</th>\n",
       "      <th>%upvote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>256602</th>\n",
       "      <td>256603</td>\n",
       "      <td>B003JA5KBW</td>\n",
       "      <td>A1KXJCXS6HFRQZ</td>\n",
       "      <td>Eric S. Olstad</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1296432000</td>\n",
       "      <td>Great vitamin pack alternative</td>\n",
       "      <td>I usually look to Emergen-C for a quick vitami...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471623</th>\n",
       "      <td>471624</td>\n",
       "      <td>B000CSBVOO</td>\n",
       "      <td>A133XHNZVQYAME</td>\n",
       "      <td>Reader</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>1162512000</td>\n",
       "      <td>Zero Transfat</td>\n",
       "      <td>To the other reviewers who were freaking out a...</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>80-100%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id   ProductId          UserId     ProfileName  \\\n",
       "256602  256603  B003JA5KBW  A1KXJCXS6HFRQZ  Eric S. Olstad   \n",
       "471623  471624  B000CSBVOO  A133XHNZVQYAME          Reader   \n",
       "\n",
       "        HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "256602                     0                       0      4  1296432000   \n",
       "471623                     8                       9      5  1162512000   \n",
       "\n",
       "                               Summary  \\\n",
       "256602  Great vitamin pack alternative   \n",
       "471623                   Zero Transfat   \n",
       "\n",
       "                                                     Text  helpfull%  %upvote  \n",
       "256602  I usually look to Emergen-C for a quick vitami...  -1.000000      NaN  \n",
       "471623  To the other reviewers who were freaking out a...   0.888889  80-100%  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=df[df['Score']!=3]  # filtering 3  as it is neutral\n",
    "df1.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20e52d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df1['Text']\n",
    "y=df1.Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be65b729",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-cb0253a32b18>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1['Score']=df1['Score'].map(y_dict)\n"
     ]
    }
   ],
   "source": [
    "y_dict={1:0,2:0,4:1,5:1}  # mapping 1,2 to zero and 4,5 to 1.\n",
    "df1['Score']=df1['Score'].map(y_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8cdbc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df1.Score  # dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50b304fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf=TfidfVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0e4a9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t=tf.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48c3e584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(525814, 114969)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90bc5d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X_t,y,test_size=.2,random_state=42,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7428fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lg=LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abb50d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40d8caee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9417284161929961"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg.score(X_train,y_train) #train score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a2a0e4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9356998183771859"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg.score(X_test,y_test)  # test score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a9e16fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml=tf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ebc7c55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1.804581953690593,\n",
       " 0.17305454273765697,\n",
       " 0.49755071924833344,\n",
       " -0.07039547896773181,\n",
       " -0.029379470746121277,\n",
       " -0.038197563109320015,\n",
       " 0.0,\n",
       " -0.04376387121009479,\n",
       " 0.011983421544646738,\n",
       " 0.0031528981403349285,\n",
       " 0.008079776103657824,\n",
       " -0.08076895385299454,\n",
       " 0.010150888776883879,\n",
       " 0.08428567202338168,\n",
       " -0.25343916025079544,\n",
       " 0.018405251788438257,\n",
       " 0.010581782028151758,\n",
       " 0.0,\n",
       " 0.005565977124073206,\n",
       " 0.009076514694552909,\n",
       " 0.0057701607674022545,\n",
       " 0.04964911363695034,\n",
       " 0.03142604629276632,\n",
       " 0.03142604629276632,\n",
       " 0.03142604629276632,\n",
       " 0.03142604629276632,\n",
       " 0.03142604629276632,\n",
       " 0.03142604629276632,\n",
       " 0.03142604629276632,\n",
       " 0.03142604629276632,\n",
       " 0.03142604629276632,\n",
       " -0.008374985245172852,\n",
       " 0.03677079746726157,\n",
       " 0.0023993698713250357,\n",
       " -0.11131416489227003,\n",
       " -0.009431726839828337,\n",
       " 0.0021845594420766895,\n",
       " 0.01756524815510021,\n",
       " 0.08234156315086909,\n",
       " 0.0016449247076482916,\n",
       " -0.03058896472082081,\n",
       " 0.017471080299244646,\n",
       " 0.005118040571361659,\n",
       " 0.036491260034421935,\n",
       " -0.0651632181187657,\n",
       " 0.0003372714393192589,\n",
       " 0.025802639916567942,\n",
       " 0.0028503478167862515,\n",
       " 0.0023993698713250357,\n",
       " -0.01652409474635151,\n",
       " 0.01855882769610742,\n",
       " 0.0071756277763887905,\n",
       " 0.0071756277763887905,\n",
       " 0.05331049520814875,\n",
       " 0.0015635234667924897,\n",
       " 0.0007763634362759339,\n",
       " 0.0,\n",
       " -0.0010620874039033114,\n",
       " -0.41743934077525763,\n",
       " 0.08833849062676252,\n",
       " -0.0699714714271238,\n",
       " 0.023783995578502068,\n",
       " 0.01861714312854937,\n",
       " 0.005286041614758499,\n",
       " 0.004281258002553643,\n",
       " 0.09643044408857934,\n",
       " 0.009090200662626714,\n",
       " 0.018457599591988755,\n",
       " -0.056371476899567834,\n",
       " 0.28190290874538226,\n",
       " 0.03884255754402576,\n",
       " 0.016268127782620892,\n",
       " -0.006722306017034801,\n",
       " -0.009008434556767302,\n",
       " 0.009194466136907227,\n",
       " 0.0019446672215258046,\n",
       " -0.009602978372171788,\n",
       " 0.03306690391952357,\n",
       " 0.03306690391952357,\n",
       " -0.07227765249512451,\n",
       " 0.0043836525343979435,\n",
       " 0.0043836525343979435,\n",
       " -0.060987917781188214,\n",
       " 0.001446427386818484,\n",
       " 0.014848089679930477,\n",
       " 0.011105222514834562,\n",
       " 0.26907236732258133,\n",
       " 0.009142920495000258,\n",
       " 0.0369535879244125,\n",
       " 0.006643117560171959,\n",
       " 0.05119934841291696,\n",
       " 0.0,\n",
       " 0.013888086503650747,\n",
       " 0.0030961017904173735,\n",
       " -0.11968607942853099,\n",
       " 0.002721056722932342,\n",
       " 0.11760660525729819,\n",
       " 0.006803720131895968,\n",
       " -0.009110345759582108,\n",
       " -0.07786065118172429,\n",
       " -0.038930325590862146,\n",
       " -0.038930325590862146,\n",
       " 0.004672430465688212,\n",
       " 0.02589255301640735,\n",
       " 0.05589401280074994,\n",
       " 0.0,\n",
       " -0.03216658501313358,\n",
       " -0.11131416489227003,\n",
       " 0.01892251711009725,\n",
       " -0.02218619536563875,\n",
       " 0.006330779488205266,\n",
       " 0.00526487239263679,\n",
       " 0.008243733631167042,\n",
       " 0.12186830629732036,\n",
       " 0.00410617295742882,\n",
       " -1.2788550278856559,\n",
       " -0.3943287803660045,\n",
       " 0.0010073009083097963,\n",
       " -0.038930325590862146,\n",
       " -0.00037191597686185996,\n",
       " -0.2213071058729636,\n",
       " -0.038930325590862146,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.01482087171307357,\n",
       " 0.00733330884844595,\n",
       " 0.00791622842340735,\n",
       " 0.02382395862237408,\n",
       " -0.11679097677258665,\n",
       " 0.00017037139983552664,\n",
       " 0.0009763732936764521,\n",
       " 0.0023116212198090935,\n",
       " 0.0,\n",
       " 0.0016914392454728268,\n",
       " 0.0,\n",
       " 0.006803720131895968,\n",
       " 0.0,\n",
       " 0.0031425745067485134,\n",
       " 0.0003372714393192589,\n",
       " -0.038930325590862146,\n",
       " 0.0,\n",
       " 0.0007657430789633749,\n",
       " -0.029091868889890286,\n",
       " -0.11679097677258665,\n",
       " 0.019351944785817116,\n",
       " -0.038930325590862146,\n",
       " 0.0105539592088639,\n",
       " 0.03791210648795569,\n",
       " 0.010305652409434015,\n",
       " -0.036163957725670465,\n",
       " 0.006090138509004651,\n",
       " 0.012142014770501416,\n",
       " -0.08076895385299454,\n",
       " 0.0,\n",
       " 0.004104358233112409,\n",
       " -0.014590771789337468,\n",
       " 0.0043836525343979435,\n",
       " 0.0,\n",
       " -0.08076895385299454,\n",
       " 0.006018636687474683,\n",
       " 0.009498671440590762,\n",
       " -0.13929714741589788,\n",
       " 0.027313751036045136,\n",
       " 0.0010293021749459277,\n",
       " -0.005156628008085325,\n",
       " 0.0005657445651323702,\n",
       " -0.12939505235817347,\n",
       " -0.7512841739260232,\n",
       " 0.1158766119789259,\n",
       " -0.07786065118172429,\n",
       " -0.038930325590862146,\n",
       " 0.0070605199464692705,\n",
       " -0.038930325590862146,\n",
       " -0.038930325590862146,\n",
       " 0.016867941341787842,\n",
       " -0.038930325590862146,\n",
       " -0.07786065118172429,\n",
       " -0.038930325590862146,\n",
       " -0.11679097677258665,\n",
       " -0.07786065118172429,\n",
       " 0.00912089737498339,\n",
       " 0.0021354943631616205,\n",
       " -0.038930325590862146,\n",
       " 0.021226987942470046,\n",
       " -0.041032833108592744,\n",
       " 0.0028837568311760606,\n",
       " 0.0635647031703932,\n",
       " 0.008326096379242032,\n",
       " 0.00392187293018709,\n",
       " 0.0635647031703932,\n",
       " 0.033535566239670085,\n",
       " 0.0,\n",
       " 0.0015964569606830366,\n",
       " 0.0,\n",
       " 0.008123615113537605,\n",
       " 0.007859866923646817,\n",
       " -0.06721086058708114,\n",
       " 0.0011999899722315499,\n",
       " 0.004354939219348281,\n",
       " 0.0,\n",
       " 0.010661763979634056,\n",
       " 0.10713122441997718,\n",
       " 0.0025329709694292377,\n",
       " 0.004881248481617761,\n",
       " 0.01117694538457923,\n",
       " 0.04367646463532805,\n",
       " 0.0,\n",
       " 0.006643117560171959,\n",
       " -0.048287329758819615,\n",
       " 0.07851637623740525,\n",
       " 0.006643117560171959,\n",
       " 0.0014029188491604843,\n",
       " 0.003635073211875435,\n",
       " -0.6886661276194275,\n",
       " 0.026572470240687835,\n",
       " -0.01671595525626451,\n",
       " -0.038930325590862146,\n",
       " 0.09760410896606224,\n",
       " 0.006643117560171959,\n",
       " -0.038930325590862146,\n",
       " -0.038930325590862146,\n",
       " 0.05755435911705631,\n",
       " -0.038930325590862146,\n",
       " 0.040533594348152666,\n",
       " -0.038930325590862146,\n",
       " -0.038930325590862146,\n",
       " 0.04367173895433382,\n",
       " 0.011742183204438577,\n",
       " 0.02589255301640735,\n",
       " -0.038930325590862146,\n",
       " -0.07786065118172429,\n",
       " -0.038930325590862146,\n",
       " -0.038930325590862146,\n",
       " -0.020516416554296372,\n",
       " -0.13100628302775058,\n",
       " -0.0921798574376806,\n",
       " 0.0009832759126074022,\n",
       " 0.0,\n",
       " 0.006943157993268749,\n",
       " 0.005510781967280468,\n",
       " -0.07663243115533641,\n",
       " 0.0,\n",
       " 0.0022805434961184127,\n",
       " 0.0008843116512338817,\n",
       " -0.03024393115530139,\n",
       " 0.013904812032349001,\n",
       " -0.11679097677258665,\n",
       " -0.038930325590862146,\n",
       " -0.07786065118172429,\n",
       " 0.006643117560171959,\n",
       " -0.1753576516081777,\n",
       " -0.038930325590862146,\n",
       " -0.038930325590862146,\n",
       " -0.038930325590862146,\n",
       " -0.07786065118172429,\n",
       " 0.013485264527994367,\n",
       " -0.07786065118172429,\n",
       " 0.11674866268287319,\n",
       " -0.09835774134467837,\n",
       " -0.038930325590862146,\n",
       " -0.07786065118172429,\n",
       " -0.07786065118172429,\n",
       " 0.006643117560171959,\n",
       " -0.025898183401519764,\n",
       " 0.01003086819021407,\n",
       " 0.0,\n",
       " -0.060793162132910944,\n",
       " 0.0,\n",
       " 0.025809258422778928,\n",
       " 0.0,\n",
       " 0.006157055806205246,\n",
       " 0.003261684706628591,\n",
       " 0.010258274057184071,\n",
       " 0.05139311892805675,\n",
       " 0.0,\n",
       " 0.03397708974415386,\n",
       " 0.001446427386818484,\n",
       " -0.036716214198130176,\n",
       " 0.00598493011006465,\n",
       " 0.0018414991892864178,\n",
       " 0.0,\n",
       " 1.88624381290131,\n",
       " 0.0033701196060141837,\n",
       " -0.10701023342894853,\n",
       " 0.01267844355124367,\n",
       " -0.03653228353223972,\n",
       " -0.2614352461124866,\n",
       " 0.0034552548907827987,\n",
       " 0.07353727288628967,\n",
       " 0.0,\n",
       " -0.038930325590862146,\n",
       " 0.019752044978449842,\n",
       " -0.038930325590862146,\n",
       " -0.038930325590862146,\n",
       " -0.038930325590862146,\n",
       " 0.0008313358930892163,\n",
       " 0.0004638491716157369,\n",
       " 0.007584665895730244,\n",
       " 0.04367646463532805,\n",
       " 0.016716089412264667,\n",
       " 0.011116064897047214,\n",
       " 0.00025276348321421867,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.06721086058708114,\n",
       " -0.20323383444265666,\n",
       " 0.00556417229272312,\n",
       " 0.0,\n",
       " 0.009296579633535923,\n",
       " 0.00416143469082802,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0002857157507575623,\n",
       " 0.0,\n",
       " -0.044615756934822885,\n",
       " 0.024063760108811928,\n",
       " 0.0025329709694292377,\n",
       " 0.009011680069357852,\n",
       " 0.001930884165790571,\n",
       " 0.0,\n",
       " 0.0034906608277497915,\n",
       " 0.040107198261610445,\n",
       " 0.0,\n",
       " 0.005118524006620788,\n",
       " 0.0,\n",
       " 0.006623514137398352,\n",
       " 0.03162250850429105,\n",
       " 0.013888086503650747,\n",
       " -0.07417024402307443,\n",
       " -0.5557631764866131,\n",
       " 0.010158383546053858,\n",
       " 0.004044664175514772,\n",
       " 0.0015964569606830366,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.013589912773841693,\n",
       " -0.06890349557520593,\n",
       " 0.01993948539851974,\n",
       " 0.0003213029568771209,\n",
       " 0.0025329709694292377,\n",
       " -0.014380812372182326,\n",
       " 0.0092571231178934,\n",
       " 0.009439092524079736,\n",
       " 0.020023588334910594,\n",
       " 0.006168261264720106,\n",
       " 0.006168261264720106,\n",
       " 0.005418113851634119,\n",
       " 0.0,\n",
       " 0.0032177570276610682,\n",
       " 0.0014891214413039519,\n",
       " 0.03162250850429105,\n",
       " 0.010437004645799466,\n",
       " -0.06721086058708114,\n",
       " 0.0043836525343979435,\n",
       " 0.006643117560171959,\n",
       " 0.0023993698713250357,\n",
       " -0.03434150041867042,\n",
       " -0.2564475471685433,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.030921850855903626,\n",
       " 0.008776489394374982,\n",
       " 0.0019801996147572358,\n",
       " 0.019972167309949978,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.04064619108114613,\n",
       " -0.600354520136838,\n",
       " 0.03276574175165816,\n",
       " -0.14581293152170577,\n",
       " -0.24654447285568393,\n",
       " 0.006643117560171959,\n",
       " 0.006803720131895968,\n",
       " -0.09217193188288891,\n",
       " 0.0006386427684638565,\n",
       " 0.007151697201883924,\n",
       " 0.0185142462357868,\n",
       " -0.32954728021810603,\n",
       " -0.0076217951467930254,\n",
       " -0.0076217951467930254,\n",
       " 0.02982311657774468,\n",
       " -0.05786394252674584,\n",
       " 0.02040038630287272,\n",
       " 0.02040038630287272,\n",
       " 0.008963779248695942,\n",
       " 0.024542071588674778,\n",
       " 0.0007428531189897291,\n",
       " -0.2072446653584364,\n",
       " 0.06167005885549784,\n",
       " -0.06721086058708114,\n",
       " 0.031475261562463004,\n",
       " 0.002667622957259247,\n",
       " 0.0012760956245112336,\n",
       " -0.006463055033086547,\n",
       " 0.011050639439636675,\n",
       " 0.0009233525296912508,\n",
       " 0.0028503478167862515,\n",
       " 0.00044995962287754153,\n",
       " -0.3298143395991048,\n",
       " 0.05581561334667522,\n",
       " 0.0003414167269232728,\n",
       " 0.036491260034421935,\n",
       " 0.021952904862748736,\n",
       " 0.011920712441356297,\n",
       " 0.0,\n",
       " 0.011521536010397354,\n",
       " 0.05294145075594506,\n",
       " 0.015547560625016157,\n",
       " 0.009967014094515936,\n",
       " 0.009967014094515936,\n",
       " 0.0031800495521944975,\n",
       " 0.010613847904695288,\n",
       " 0.001446427386818484,\n",
       " 0.0037329590086501727,\n",
       " 0.046269969070884334,\n",
       " 0.052922773606815736,\n",
       " 0.0,\n",
       " -0.044293026321258804,\n",
       " 1.8445536958871407,\n",
       " 0.002452388454148031,\n",
       " 0.002452388454148031,\n",
       " 0.00859590409923526,\n",
       " 0.02297620248491037,\n",
       " 0.005473146277311201,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.04040868602891607,\n",
       " 0.010946292554622402,\n",
       " 0.5574563273735337,\n",
       " 0.005473146277311201,\n",
       " 0.012124367331344576,\n",
       " 0.03318527042830123,\n",
       " -0.17886238329618523,\n",
       " 0.04192602144809176,\n",
       " 0.01909835370974813,\n",
       " -0.058314369564380415,\n",
       " 0.004567193040491434,\n",
       " 0.0,\n",
       " 0.0037389539460716634,\n",
       " 0.023383358208891026,\n",
       " -0.11386660182054469,\n",
       " 0.003950484960565211,\n",
       " -0.12972090388927027,\n",
       " 0.002598831370320299,\n",
       " 1.0013800251394367,\n",
       " -1.2087880195169514,\n",
       " 0.1773697642083471,\n",
       " -0.23078382397172414,\n",
       " 0.12182211999250521,\n",
       " 0.030291718141067996,\n",
       " 0.017167520390534242,\n",
       " 0.026953720770645954,\n",
       " -0.004276186029345437,\n",
       " 0.03230105828448874,\n",
       " 0.0,\n",
       " 0.002000017891605038,\n",
       " -0.006268099518341256,\n",
       " 0.0,\n",
       " -0.12139434591265091,\n",
       " 0.0,\n",
       " 0.06306615492921236,\n",
       " 0.02018237725093122,\n",
       " -0.06014014459872036,\n",
       " 0.06848238255291692,\n",
       " 0.017219901442677665,\n",
       " 0.005341047677184646,\n",
       " 0.03830732991966319,\n",
       " 0.0,\n",
       " -0.07547594098443972,\n",
       " 0.008433970670893921,\n",
       " 0.039857738474391904,\n",
       " 0.021575347341122965,\n",
       " 0.036394313887516024,\n",
       " 0.05911537025805912,\n",
       " 0.16763157732406408,\n",
       " -0.05879629326625694,\n",
       " 0.25051634848213783,\n",
       " 0.020027279292433735,\n",
       " -0.20558006677833718,\n",
       " 0.005386541453059511,\n",
       " -0.012205905682541606,\n",
       " 0.14105817407939128,\n",
       " 0.011443289467368367,\n",
       " -0.306790398523528,\n",
       " 0.0,\n",
       " 0.006074996302605856,\n",
       " 0.006643117560171959,\n",
       " 0.03831402307948819,\n",
       " 0.16437555140546614,\n",
       " 0.0038519373548921375,\n",
       " -0.7148752299629787,\n",
       " 0.010150888776883879,\n",
       " 0.03671772741453735,\n",
       " -0.19418291568487323,\n",
       " 0.09177701693170451,\n",
       " 0.008911156823730216,\n",
       " 0.005380375968677419,\n",
       " 0.06293438642557315,\n",
       " 0.06293438642557315,\n",
       " 0.002602575075157312,\n",
       " 0.0025867552305278386,\n",
       " -0.07058682561541464,\n",
       " -0.4992219702459391,\n",
       " 0.012953731407383973,\n",
       " -0.06376213473252548,\n",
       " 0.0,\n",
       " -0.026419217875643223,\n",
       " 0.020095639834526516,\n",
       " -0.025122015392326252,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.004707965796737573,\n",
       " -0.37307973004584877,\n",
       " -0.019340255260060796,\n",
       " -0.27251227913603526,\n",
       " -0.06948064888501065,\n",
       " 0.0301660506670933,\n",
       " 0.007638139696091529,\n",
       " 0.009925740551111605,\n",
       " 0.00821185774059519,\n",
       " 0.015422924028138614,\n",
       " -0.06236368509684571,\n",
       " -0.07786065118172429,\n",
       " -0.03713170163735546,\n",
       " -0.4377069598593216,\n",
       " -0.11324113090766898,\n",
       " 0.010582277116236374,\n",
       " 0.022749845636294286,\n",
       " 0.02783293796629571,\n",
       " 0.007747140378991229,\n",
       " 0.04794111742467104,\n",
       " -0.038930325590862146,\n",
       " 0.004593052799182175,\n",
       " 0.15211051736748074,\n",
       " 0.027362887603399177,\n",
       " 0.0047654182535608386,\n",
       " -0.019205229485557702,\n",
       " 0.0,\n",
       " -0.32046947656809655,\n",
       " 0.001459404580990932,\n",
       " -0.05127654488393513,\n",
       " 0.0,\n",
       " -0.8236191711560651,\n",
       " 0.050414228585826615,\n",
       " -0.07823395424648802,\n",
       " -0.12695908794121258,\n",
       " 0.006032433888181204,\n",
       " 0.01677049081989007,\n",
       " -0.14486009128056623,\n",
       " 0.06370116420832951,\n",
       " 0.009350240642699813,\n",
       " 0.06589976308641103,\n",
       " 0.018133700416086584,\n",
       " -0.010906078607453322,\n",
       " 0.0,\n",
       " 0.11737881494040155,\n",
       " 0.0904865083227181,\n",
       " 0.024129735552724817,\n",
       " 0.012064867776362408,\n",
       " 0.018097301664543666,\n",
       " 0.07238920665817467,\n",
       " 0.015270384457828967,\n",
       " 0.024129735552724817,\n",
       " -0.10522497928389916,\n",
       " 0.05669175107164756,\n",
       " 0.01725874398915721,\n",
       " 0.07021173922868328,\n",
       " 0.044525275803114106,\n",
       " 0.006032433888181204,\n",
       " 0.030162169440905922,\n",
       " 0.060324338881811844,\n",
       " 0.018097301664543666,\n",
       " 0.024129735552724817,\n",
       " 0.03619460332908733,\n",
       " 0.0006388757630921388,\n",
       " 0.019569634716759415,\n",
       " -0.20915234251462095,\n",
       " 0.07842164054635531,\n",
       " 0.03619460332908733,\n",
       " 0.006032433888181204,\n",
       " 0.006032433888181204,\n",
       " 0.018097301664543666,\n",
       " -0.03198881212571208,\n",
       " 0.018097301664543666,\n",
       " 0.006032433888181204,\n",
       " 0.0,\n",
       " 0.06944934541767987,\n",
       " 0.175128311175689,\n",
       " 0.01743846403758104,\n",
       " 0.0,\n",
       " -0.017263979597747354,\n",
       " 0.0,\n",
       " 0.028750418784927654,\n",
       " -0.341308711205137,\n",
       " 0.006293067950522785,\n",
       " 0.33366009167477967,\n",
       " 0.01255144570846967,\n",
       " 0.03420547149414347,\n",
       " 0.005734953289069218,\n",
       " 0.005977314102567715,\n",
       " 0.005384848370234246,\n",
       " 0.011633651470286312,\n",
       " 0.016982411225503122,\n",
       " -0.12091208494354476,\n",
       " 0.01910958660415591,\n",
       " 0.006237051694770512,\n",
       " -0.0012655532029969348,\n",
       " 0.28646900073038106,\n",
       " 0.0,\n",
       " 0.06266163812922282,\n",
       " 0.31558925714155306,\n",
       " 0.01288184172836055,\n",
       " 0.00040953726684257784,\n",
       " 0.010150888776883879,\n",
       " 0.19764178476504354,\n",
       " 0.008433970670893921,\n",
       " 0.0008099236592469964,\n",
       " 0.05253277985542264,\n",
       " -0.5928561329747326,\n",
       " 0.005788168536527152,\n",
       " 0.0011760555809643433,\n",
       " -0.16118358015887232,\n",
       " 0.019673743667419693,\n",
       " 0.2203895421106182,\n",
       " 0.027851042758063323,\n",
       " 0.42766295943088645,\n",
       " 0.002401701562105353,\n",
       " 0.02233967768659483,\n",
       " 0.022995087916806548,\n",
       " 0.010039367559258142,\n",
       " 0.002957442821668801,\n",
       " -0.7573672877173226,\n",
       " 0.001725172938712933,\n",
       " 0.03500894987938306,\n",
       " 0.0004739534861062878,\n",
       " 0.004027986814081065,\n",
       " 0.0,\n",
       " -0.17731153956403864,\n",
       " -0.09321828956855115,\n",
       " 0.0,\n",
       " 0.006816844385620108,\n",
       " 0.000617121014325958,\n",
       " 0.31154518198161185,\n",
       " 0.0023219276475517875,\n",
       " 0.0,\n",
       " -0.03800459668692233,\n",
       " 0.0,\n",
       " 0.0013685260658635057,\n",
       " 0.0029917988102013213,\n",
       " 0.025170334176481252,\n",
       " 0.25346381891369385,\n",
       " 0.30752301374163254,\n",
       " 0.0,\n",
       " -0.20786861873943827,\n",
       " 0.0667915514776278,\n",
       " 0.3985995422139253,\n",
       " 0.0,\n",
       " 0.008155352687074601,\n",
       " 0.027404521602583994,\n",
       " 0.006032433888181204,\n",
       " 0.012064867776362408,\n",
       " 0.0,\n",
       " -0.047076516442220245,\n",
       " -0.023538258221110123,\n",
       " 0.035280176910786616,\n",
       " 0.006803720131895968,\n",
       " -0.038930325590862146,\n",
       " -0.07016062855956474,\n",
       " -0.2189539303939283,\n",
       " 0.02546920955660891,\n",
       " 0.0,\n",
       " 0.11907752059957276,\n",
       " 0.10552027184297205,\n",
       " 0.09441564983293799,\n",
       " 0.03430436986252168,\n",
       " 0.004371924119859957,\n",
       " 0.007938462793390758,\n",
       " 0.006803720131895968,\n",
       " 0.0,\n",
       " -0.5220533559170834,\n",
       " 0.0,\n",
       " 0.11060201609321893,\n",
       " -0.038930325590862146,\n",
       " -0.38534625856441196,\n",
       " 0.014100167330732286,\n",
       " 0.0,\n",
       " -0.11118738810559298,\n",
       " -0.15154421116109215,\n",
       " -0.11118738810559298,\n",
       " -0.03434150041867042,\n",
       " 0.0,\n",
       " -0.038930325590862146,\n",
       " 0.015265064207628258,\n",
       " 0.42534359107741176,\n",
       " -0.07063809468213691,\n",
       " 0.016667934471115243,\n",
       " -0.08383885209132443,\n",
       " 0.028395941515876773,\n",
       " -0.47761823660895286,\n",
       " -0.040041054941080484,\n",
       " 0.06632761482595848,\n",
       " 0.031536068655439196,\n",
       " 0.025651139458985398,\n",
       " 0.1495762501199054,\n",
       " 0.0015625167942181033,\n",
       " 0.04015419682502551,\n",
       " 0.003732050673375891,\n",
       " 0.056332344261184594,\n",
       " -0.12775227342525458,\n",
       " 0.029238812705615017,\n",
       " 0.0857827264575809,\n",
       " 0.004738503994742695,\n",
       " 0.0,\n",
       " -0.07551394511004486,\n",
       " -0.07663243115533641,\n",
       " 0.0,\n",
       " 0.005700991970977738,\n",
       " 0.1008096638334076,\n",
       " 0.021575347341122965,\n",
       " 0.034914353727102394,\n",
       " 0.09415995124000352,\n",
       " -0.020609008320492634,\n",
       " -0.03198881212571208,\n",
       " 0.0029944598832776855,\n",
       " -0.4897880905024573,\n",
       " 0.001017961252051938,\n",
       " 0.0,\n",
       " -0.09898557972545581,\n",
       " 0.19881281104664064,\n",
       " -0.17159077799627903,\n",
       " 0.18388098888662166,\n",
       " -0.044535466665270196,\n",
       " 0.00770137554427256,\n",
       " 0.0039979647701690645,\n",
       " 0.0,\n",
       " 0.13433201819741236,\n",
       " 0.0,\n",
       " 0.0034358046550126567,\n",
       " 0.017057136452564545,\n",
       " 0.3350470712001505,\n",
       " -0.057544003284419526,\n",
       " 0.09443662748034369,\n",
       " 0.010150888776883879,\n",
       " 0.13474315888846805,\n",
       " 0.0,\n",
       " 0.019377726321986616,\n",
       " -0.14557526559775652,\n",
       " -0.22821332038821462,\n",
       " -0.15864503073972874,\n",
       " -0.04885005206311532,\n",
       " 0.01852382216636332,\n",
       " -0.16745429068642528,\n",
       " 0.09089074593974299,\n",
       " 0.021069753829757613,\n",
       " 0.0,\n",
       " 0.2741403119037773,\n",
       " 0.12846549182587635,\n",
       " 0.005112082623621497,\n",
       " -0.7457235545734351,\n",
       " -0.529820875936012,\n",
       " 0.12552637216252954,\n",
       " -0.04398319366471785,\n",
       " -0.00268654342738457,\n",
       " -0.1938854972218195,\n",
       " -0.06167922582231472,\n",
       " 0.00855472996525394,\n",
       " -0.05370107801237052,\n",
       " -0.038930325590862146,\n",
       " -0.038930325590862146,\n",
       " -0.001820706462110212,\n",
       " 0.0,\n",
       " -0.04938681181846074,\n",
       " -0.03434150041867042,\n",
       " 0.018918241627710154,\n",
       " -0.03653228353223972,\n",
       " 0.0,\n",
       " 0.07851218942503724,\n",
       " 0.006923926937171306,\n",
       " 0.09073749737919241,\n",
       " 0.6085379227394062,\n",
       " 0.003658063357280635,\n",
       " 0.007377941438563177,\n",
       " 0.0755327478014448,\n",
       " 2.112466602716821,\n",
       " 0.0012503997141563008,\n",
       " 0.09508178343167746,\n",
       " 0.00809065700025503,\n",
       " -0.07067922084971971,\n",
       " 0.06261381437639753,\n",
       " 0.10468361127266598,\n",
       " -0.019205229485557702,\n",
       " 0.05315028322791446,\n",
       " 0.05353847171584151,\n",
       " 0.1222146919422845,\n",
       " 0.0028912477295938667,\n",
       " 0.06739913011782472,\n",
       " -0.10322304632763901,\n",
       " 0.0,\n",
       " 0.00410617295742882,\n",
       " 0.03286748994178749,\n",
       " 0.0028912477295938667,\n",
       " -0.0636805247721271,\n",
       " 0.0,\n",
       " 0.0028912477295938667,\n",
       " 0.003882975475985747,\n",
       " 0.004790016918126788,\n",
       " 0.004790016918126788,\n",
       " 0.04085435243361695,\n",
       " 0.3092236483318273,\n",
       " -0.2349882096175097,\n",
       " 0.0,\n",
       " 0.09469560803787662,\n",
       " 0.0,\n",
       " 0.19647246149198316,\n",
       " 0.0,\n",
       " 0.00040780220858473566,\n",
       " -0.1235494433071846,\n",
       " 0.01136134949451943,\n",
       " 0.0007763634362759339,\n",
       " 0.046396529149676985,\n",
       " -0.04878661422562327,\n",
       " 0.003173756817532298,\n",
       " 0.005922384493796499,\n",
       " 0.0028838270249013117,\n",
       " -0.025622856385053693,\n",
       " 0.04955376184678955,\n",
       " 0.023849107819743618,\n",
       " -0.01330679833149122,\n",
       " 0.0,\n",
       " 0.007411212290397435,\n",
       " 0.00796277824035737,\n",
       " 0.0,\n",
       " 0.0015455291559152563,\n",
       " 0.024272705813528486,\n",
       " 0.0,\n",
       " 0.048750470834447994,\n",
       " -0.13791434202482303,\n",
       " 0.0,\n",
       " -0.04939579196107249,\n",
       " -0.09506128582255911,\n",
       " 0.05542240893121429,\n",
       " 0.015293069075432182,\n",
       " -0.508359316134836,\n",
       " -0.07046480520937184,\n",
       " 0.006536885573893221,\n",
       " -0.07046480520937184,\n",
       " -0.09510644732405935,\n",
       " 0.0030243789352963333,\n",
       " 0.018782578987811983,\n",
       " -0.13791434202482303,\n",
       " 0.01950273663306891,\n",
       " 0.008802784149324335,\n",
       " 0.00044995962287754153,\n",
       " 0.0057660659054027934,\n",
       " 0.03730632881742674,\n",
       " 0.004063510481419144,\n",
       " 0.024970874375284446,\n",
       " -0.3787196494525909,\n",
       " 0.054617046591606884,\n",
       " 0.014755134114332834,\n",
       " 0.02477706690493228,\n",
       " 0.017485673144464586,\n",
       " 0.023701595819802006,\n",
       " 0.0,\n",
       " 0.07851218942503724,\n",
       " 0.0044852617282151825,\n",
       " 0.03829509091562588,\n",
       " 0.01870105853570167,\n",
       " 0.012856463147267915,\n",
       " 0.011633651470286312,\n",
       " -0.2897088109041125,\n",
       " 0.0,\n",
       " 0.008471590808119149,\n",
       " 0.0039940538258917595,\n",
       " -0.24654447285568393,\n",
       " 0.0026565829923032836,\n",
       " 0.009269481019128186,\n",
       " 0.048686151507123535,\n",
       " 0.11225491497184296,\n",
       " 0.0,\n",
       " 0.005189058759051232,\n",
       " 0.0026411474976838576,\n",
       " 0.003205232736450152,\n",
       " 0.016101391809645437,\n",
       " -0.15391655616899974,\n",
       " 0.6665254064296483,\n",
       " 0.25044170634945145,\n",
       " -0.5622128557463907,\n",
       " -0.18030097311214213,\n",
       " -0.6328698820107925,\n",
       " 0.0,\n",
       " 0.017617740449711053,\n",
       " 0.15606286691504292,\n",
       " 0.0040860334084038726,\n",
       " 0.00764005483497461,\n",
       " 0.14030608468469494,\n",
       " 0.05000803005474853,\n",
       " -0.15160128126178624,\n",
       " 0.012359483694027052,\n",
       " -0.21052062946566213,\n",
       " 0.005050299707347241,\n",
       " 0.11661499014867781,\n",
       " 0.03230765629846704,\n",
       " 0.021974881294229517,\n",
       " 0.0,\n",
       " -0.004076251004195529,\n",
       " 0.007512300160603034,\n",
       " 0.028865397218542234,\n",
       " 0.009995102776712204,\n",
       " -0.08949853175846273,\n",
       " 0.1585816370071965,\n",
       " -0.01337642050924335,\n",
       " 0.053611590696022685,\n",
       " 0.004746283993115916,\n",
       " 0.008313466368669658,\n",
       " 0.003917260132425335,\n",
       " 0.0061460584564273875,\n",
       " -0.011156133212072693,\n",
       " 0.0025341627946254136,\n",
       " 0.07156429598988426,\n",
       " 0.0,\n",
       " 0.03174538212674292,\n",
       " -0.16489462940361427,\n",
       " 0.026080328781615817,\n",
       " 0.026734709247272097,\n",
       " 0.0043678446354496045,\n",
       " -1.07686388609159,\n",
       " 1.1823421408726624,\n",
       " -0.46596640916581467,\n",
       " 0.06954176154629882,\n",
       " 0.11404540881064283,\n",
       " 0.007653211861655163,\n",
       " 0.0022059468013179654,\n",
       " 0.0,\n",
       " 0.003037153696790041,\n",
       " 0.0,\n",
       " 0.006517398517431877,\n",
       " -0.03246113882124134,\n",
       " -0.03246113882124134,\n",
       " -0.6847389997304024,\n",
       " 0.002452388454148031,\n",
       " 0.007393966877246517,\n",
       " 0.0033191222248613143,\n",
       " 0.01566586795893282,\n",
       " 0.37047895435317446,\n",
       " -0.1031717681626654,\n",
       " 0.0652835594498088,\n",
       " 0.0065488145903738635,\n",
       " 0.0,\n",
       " 0.27726064176847953,\n",
       " -0.1340403948568668,\n",
       " 0.007248644941949276,\n",
       " -0.1413925047875069,\n",
       " 0.015365541990171705,\n",
       " -0.05935442474670034,\n",
       " 0.0474981779268024,\n",
       " 0.007106187878682017,\n",
       " 0.014212375757364034,\n",
       " 0.007106187878682017,\n",
       " 0.09678169870317378,\n",
       " 0.007823316152784398,\n",
       " 0.0,\n",
       " 0.003091196328856736,\n",
       " 0.03364283897889207,\n",
       " 0.03962957652632945,\n",
       " -0.5777925066621521,\n",
       " 0.024149315918201946,\n",
       " 0.0,\n",
       " -0.08867882189587009,\n",
       " -0.08867882189587009,\n",
       " 0.007075045012785786,\n",
       " 0.06844546608851863,\n",
       " 0.012568067234453459,\n",
       " 0.005350326503205878,\n",
       " 0.0283484110951465,\n",
       " -0.0664455553480486,\n",
       " -0.08893685439030714,\n",
       " 0.0024840061613673418,\n",
       " 0.0,\n",
       " 0.042299959716042966,\n",
       " -0.07862104518063237,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0015300914414505967,\n",
       " -0.2312228130653917,\n",
       " 0.00351794613585746,\n",
       " 0.01642649354055959,\n",
       " 0.0,\n",
       " -0.10215468191297959,\n",
       " 0.0,\n",
       " -0.06003206345717107,\n",
       " 0.0008719173907654864,\n",
       " 0.0178651459289946,\n",
       " 0.0178651459289946,\n",
       " 0.008411298439507723,\n",
       " 0.0005650420895801659,\n",
       " -0.05649593351559618,\n",
       " 0.05043573667866582,\n",
       " 0.014613773414147137,\n",
       " ...]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg.coef_.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f7f59b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00</td>\n",
       "      <td>-1.804582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000</td>\n",
       "      <td>0.173055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000</td>\n",
       "      <td>0.497551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000001</td>\n",
       "      <td>-0.070395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00001</td>\n",
       "      <td>-0.029379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114964</th>\n",
       "      <td>çaykur</td>\n",
       "      <td>0.003489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114965</th>\n",
       "      <td>çelem</td>\n",
       "      <td>-0.073346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114966</th>\n",
       "      <td>être</td>\n",
       "      <td>0.034716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114967</th>\n",
       "      <td>île</td>\n",
       "      <td>0.039794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114968</th>\n",
       "      <td>ît</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114969 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          word  coefficient\n",
       "0           00    -1.804582\n",
       "1          000     0.173055\n",
       "2         0000     0.497551\n",
       "3       000001    -0.070395\n",
       "4        00001    -0.029379\n",
       "...        ...          ...\n",
       "114964  çaykur     0.003489\n",
       "114965   çelem    -0.073346\n",
       "114966    être     0.034716\n",
       "114967     île     0.039794\n",
       "114968      ît     0.000000\n",
       "\n",
       "[114969 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_coeff=pd.DataFrame({'word':ml,'coefficient':lg.coef_.tolist()[0]})\n",
    "df_coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9de945e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51300         great\n",
       "35641     delicious\n",
       "20308          best\n",
       "78889       perfect\n",
       "43222     excellent\n",
       "65108         loves\n",
       "54309        highly\n",
       "112883    wonderful\n",
       "65072          love\n",
       "7821        amazing\n",
       "Name: word, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top  10  positive words  \n",
    "df_coeff.sort_values(by='coefficient',ascending=False)['word'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb467d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88350             return\n",
       "103640             threw\n",
       "107553     unfortunately\n",
       "55168           horrible\n",
       "37556       disappointed\n",
       "11000              awful\n",
       "37563     disappointment\n",
       "102845          terrible\n",
       "37560      disappointing\n",
       "113164             worst\n",
       "Name: word, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top  10 negative words\n",
    "df_coeff.sort_values(by='coefficient',ascending=False)['word'].tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6d8c1fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.86      0.77     13225\n",
      "           1       0.98      0.95      0.96     91938\n",
      "\n",
      "    accuracy                           0.94    105163\n",
      "   macro avg       0.84      0.90      0.87    105163\n",
      "weighted avg       0.94      0.94      0.94    105163\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix,accuracy_score,classification_report\n",
    "print(classification_report(lg.predict(X_test),y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "090919c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(confusion_matrix(y_test,lg.predict(X_test)),annot=True,fmt='d',cmap=\"YlGnBu\",\n",
    "            cbar=False,annot_kws={'fontweight':'bold','size':15})\n",
    "plt.xlabel('predicted')\n",
    "plt.ylabel('actual');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130512b5",
   "metadata": {},
   "source": [
    "# Evaluating model on the basis of upvote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1e3e2a1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['80-100%', NaN, '60-80%', 'empty', '40-60%', '20-40%', '0-20%']\n",
       "Categories (6, object): ['empty' < '0-20%' < '20-40%' < '40-60%' < '60-80%' < '80-100%']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['%upvote'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6674fb92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>helpfull%</th>\n",
       "      <th>%upvote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>80-100%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>80-100%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>80-100%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>B000E7L2R4</td>\n",
       "      <td>A1MZYO9TZK0BBI</td>\n",
       "      <td>R. James</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1322006400</td>\n",
       "      <td>Yay Barley</td>\n",
       "      <td>Right now I'm mostly just sprouting this so my...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>80-100%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>B0001PB9FE</td>\n",
       "      <td>A3HDKO7OW0QNK4</td>\n",
       "      <td>Canadian Fan</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1107820800</td>\n",
       "      <td>The Best Hot Sauce in the World</td>\n",
       "      <td>I don't know if it's the cactus or the tequila...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>80-100%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Id   ProductId          UserId                      ProfileName  \\\n",
       "0    1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "2    3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3    4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "8    9  B000E7L2R4  A1MZYO9TZK0BBI                         R. James   \n",
       "10  11  B0001PB9FE  A3HDKO7OW0QNK4                     Canadian Fan   \n",
       "\n",
       "    HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                      1                       1      5  1303862400   \n",
       "2                      1                       1      4  1219017600   \n",
       "3                      3                       3      2  1307923200   \n",
       "8                      1                       1      5  1322006400   \n",
       "10                     1                       1      5  1107820800   \n",
       "\n",
       "                            Summary  \\\n",
       "0             Good Quality Dog Food   \n",
       "2             \"Delight\" says it all   \n",
       "3                    Cough Medicine   \n",
       "8                        Yay Barley   \n",
       "10  The Best Hot Sauce in the World   \n",
       "\n",
       "                                                 Text  helpfull%  %upvote  \n",
       "0   I have bought several of the Vitality canned d...        1.0  80-100%  \n",
       "2   This is a confection that has been around a fe...        1.0  80-100%  \n",
       "3   If you are looking for the secret ingredient i...        1.0  80-100%  \n",
       "8   Right now I'm mostly just sprouting this so my...        1.0  80-100%  \n",
       "10  I don't know if it's the cactus or the tequila...        1.0  80-100%  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=df[df['%upvote'].isin(['80-100%', '60-80%', '20-40%', '0-20%'])]  # removing neutral,40-60%\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "82b3d46d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator',\n",
       "       'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'Text',\n",
       "       'helpfull%', '%upvote'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "428d4e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['80-100%', '60-80%', '20-40%', '0-20%']\n",
       "Categories (4, object): ['0-20%' < '20-40%' < '60-80%' < '80-100%']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['%upvote'].unique()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5420bf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict={'80-100%':1, '60-80%':1, '20-40%':0, '0-20%':0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9dedd65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-68-538eb7cf3c3a>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['%upvote']=df['%upvote'].map(my_dict)\n"
     ]
    }
   ],
   "source": [
    "data['%upvote']=df['%upvote'].map(my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3392782e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=data['%upvote']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "386479eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    223029\n",
       "0.0     15079\n",
       "Name: %upvote, dtype: int64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8decb09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_s=tf.fit_transform(data['Text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813bb50d",
   "metadata": {},
   "source": [
    "# IMbalnce data handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3c6bee2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "rs= RandomOverSampler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bd367299",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=rs.fit_resample(X_s,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f6b845b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    223029\n",
       "1.0    223029\n",
       "Name: %upvote, dtype: int64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()  # data set is balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4225c18f",
   "metadata": {},
   "source": [
    "# model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b12fdf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param={'C':10.0**np.arange(-3,2),'penalty':['l1','l2']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "637df0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs=GridSearchCV(lg,cv=5,param_grid=param,verbose=1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3ad90e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan 0.72811608        nan 0.75926898        nan 0.80881635\n",
      "        nan 0.86751275        nan 0.90079765]\n",
      "  warnings.warn(\n",
      "C:\\Users\\Sabin Sapkota\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=LogisticRegression(),\n",
       "             param_grid={'C': array([1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01]),\n",
       "                         'penalty': ['l1', 'l2']},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a43b0c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=.2,random_state=42,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "216eca5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., ..., 0., 1., 1.])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "337be3b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(confusion_matrix(y_test,gs.predict(X_test)),annot=True,fmt='d',cmap=\"YlGnBu\",\n",
    "            cbar=False,annot_kws={'fontweight':'bold','size':15})\n",
    "plt.xlabel('predicted')\n",
    "plt.ylabel('actual');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d1cc7688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.90      0.92     46676\n",
      "         1.0       0.90      0.94      0.92     42536\n",
      "\n",
      "    accuracy                           0.92     89212\n",
      "   macro avg       0.92      0.92      0.92     89212\n",
      "weighted avg       0.92      0.92      0.92     89212\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(gs.predict(X_test),y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
